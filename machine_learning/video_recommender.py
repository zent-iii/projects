# -*- coding: utf-8 -*-
"""video_recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CacQguB0A9eWsSZbPLlOi0a8bB6TsZ_R

#Preaparing the data

##Downloading the dataset
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("abdulwadood11220/youtube-data-for-analytics-600-rows")

print("Path to dataset files:", path)

"""##Importing the dataset"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(path + "/youtube_data.csv")
df.head()

"""##Viewing basic stats"""

for col in df.columns:
  print(col, df[col].nunique())
  print(col, df[col].isnull().sum())
  print(col, df[col].value_counts().nlargest(1))

"""##Convert the time into **seconds**"""

import re

def parse_iso8601_to_seconds(duration):
    m = re.match(r'P(?:(\d+)D)?T(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration)
    if not m:
        return 0
    days = int(m.group(1)) if m.group(1) else 0
    hours = int(m.group(2)) if m.group(2) else 0
    minutes = int(m.group(3)) if m.group(3) else 0
    seconds = int(m.group(4)) if m.group(4) else 0
    return days * 86400 + hours * 3600 + minutes * 60 + seconds

df['duration_in_seconds'] = df['duration'].apply(parse_iso8601_to_seconds)
df.head()

"""##Analyzing the missing descriptions"""

df["missing_description"] = (df['description'] == "No description available")
df["missing_description"].value_counts()

column_of_interest = ["missing_description", "view_count", "like_count", "comment_count", "duration_in_seconds"]

correlation_matrix = df[column_of_interest].corr(method = 'spearman')

sns.set_theme(style="white")
plt.figure(figsize=(8,6))
heatmap = sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar_kws={"label": "Correlation Spearman"})
heatmap.set_title("Correlation Heatmap")
plt.show()

"""##Dropping no description rows"""

df = df[df['description'] != "No description available"]
df = df.dropna(subset=["description", "view_count", "like_count", "comment_count", "duration_in_seconds", "title"])
df = df.drop(columns=["missing_description", "duration"])
df = df[df["title"].str.split().str.len() > 2]
df = df[(df['view_count'] > 100) | (df['like_count'] == 0) | (df['comment_count'] == 0)]
df = df[df['like_count'] <= df['view_count']]
df.head()

def remove_promotional_prefix(text):
    if not isinstance(text, str):
        return text

    promo_patterns = [
        r"^subscribe.*?\n",
        r"^follow me.*?\n",
        r"^check out.*?\n",
        r"^join my discord.*?\n",
        r"^support me.*?\n",
        r"^buy my merch.*?\n",
        r"^visit.*?\n",
        r"^https?://\S+\s*",
    ]

    pattern = re.compile("|".join(promo_patterns), re.IGNORECASE)

    for _ in range(3):
        match = pattern.match(text)
        if match:
            text = text[match.end():].lstrip()
        else:
            break

    return text

df["description"] = df["description"].apply(remove_promotional_prefix)

df["tagged_description"] = df["video_id"].astype(str) + " " + df["description"]
df.head()

"""#Processing the texts"""

!pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

descriptions = df['description'].tolist()
description_embeddings = model.encode(descriptions, show_progress_bar=True)

df['description_embeddings'] = list(description_embeddings)

def recommend_video_enhanced(description, df, model, top_n=5, description_weight=0.75, ratio_weight=0.25):
    input_embedding = model.encode([description])[0]
    description_similarities = np.dot(df['description_embeddings'].tolist(), input_embedding)

    df['like_view_ratio'] = df.apply(lambda row: row['like_count'] / row['view_count'] if row['view_count'] > 0 else 0, axis=1)

    min_desc_sim = description_similarities.min()
    max_desc_sim = description_similarities.max()
    normalized_description_similarities = (description_similarities - min_desc_sim) / (max_desc_sim - min_desc_sim) if (max_desc_sim - min_desc_sim) != 0 else np.zeros_like(description_similarities)

    min_ratio = df['like_view_ratio'].min()
    max_ratio = df['like_view_ratio'].max()
    normalized_like_view_ratio = (df['like_view_ratio'] - min_ratio) / (max_ratio - min_ratio) if (max_ratio - min_ratio) != 0 else np.zeros_like(df['like_view_ratio'])

    combined_scores = (description_weight * normalized_description_similarities) + (ratio_weight * normalized_like_view_ratio)

    most_similar_indices = np.argsort(combined_scores)[-top_n:][::-1]

    recommended_videos = df.iloc[most_similar_indices]
    return recommended_videos[['title', 'video_id', 'description', 'view_count', 'like_count', 'comment_count', 'like_view_ratio']]

#Example
example_description = "Learn about machine learning algorithms and data science concepts."
recommended = recommend_video_enhanced(example_description, df, model)

print(f"Recommended videos for the description: '{example_description}' (Enhanced)")
print(recommended)

first_video_description = df['description'].iloc[0]
recommended_from_video = recommend_video_enhanced(first_video_description, df, model)

print(f"\nRecommended videos based on the description of the first video (Enhanced):")
recommended_from_video